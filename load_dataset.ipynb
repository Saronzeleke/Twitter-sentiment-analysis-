{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb115284",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install Kaggle API\n",
    "!pip install kaggle\n",
    "\n",
    "# Upload kaggle.json\n",
    "from google.colab import files\n",
    "files.upload()  # Upload kaggle.json\n",
    "\n",
    "# Setup Kaggle API credentials\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download Sentiment140 dataset\n",
    "!kaggle datasets download -d kazanova/sentiment140\n",
    "!unzip sentiment140.zip\n",
    "\n",
    "# Save to sentiment140_clean.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding='ISO-8859-1', header=None)\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "df.to_csv(\"sentiment140_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbbb3c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"distilbert_finetuned\")\n",
    "SUBSET_SIZE = 50000\n",
    "\n",
    "def fine_tune_model():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    df = pd.read_csv(\"sentiment140_clean.csv\")\n",
    "    df = df.sample(n=SUBSET_SIZE, random_state=42)\n",
    "    df['label'] = df['target'].map({0: 0, 4: 1})\n",
    "    dataset = Dataset.from_pandas(df[['text', 'label']].dropna())\n",
    "    encoded_dataset = dataset.map(lambda e: tokenizer(e['text'], truncation=True, padding=True, max_length=128), batched=True)\n",
    "    encoded_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "    dataset_split = encoded_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=str(MODEL_DIR),\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        #save_strategy=\"epoch\",\n",
    "        #evaluation_strategy=\"epoch\",\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset_split[\"train\"],\n",
    "        eval_dataset=dataset_split[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(MODEL_DIR)\n",
    "    tokenizer.save_pretrained(MODEL_DIR)\n",
    "    print(\"âœ… Model and tokenizer saved in:\", MODEL_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fine_tune_model()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
